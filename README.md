# ğŸ§© Revisiting the Reliability of Language Models in Instruction-Following


![alt text](fig/image.png)

## ğŸ“˜ Overview

This repository provides a reproducible pipeline for studying **the nuance-oriented reliability** of large language models in instruction-following.
It includes all stages â€” from **data augmentation** and **code-assisted validity checks** to **automatic evaluation** on IFEval and IFEval++ benchmarks.


## ğŸ“‚ Directory Structure
```bash
Reliable-IF/
â”œâ”€â”€ assets/                         # Original and augmented datasets
â”‚   â”œâ”€â”€ ifeval_original.jsonl       # Original IFEval dataset
â”‚   â”œâ”€â”€ ifeval_original_fix.jsonl   # Cleaned/processed data
â”‚   â””â”€â”€ ifeval_pp_verified.jsonl             # Augmented dataset
â”‚
â”œâ”€â”€ data_synthesis/                # Data augmentation & sanity check
â”‚   â”œâ”€â”€ exp1_rephrase/             # Rephrase experiments
â”‚   â”œâ”€â”€ exp2_add_distractor/       # Add distractors
â”‚   â”œâ”€â”€ exp3_constraint-task_reconfiguration/ 
â”‚   â””â”€â”€ data_checker.py            # Validity check script
â”‚
â”œâ”€â”€ src/                            # Source code for IFEval
â”‚   â”œâ”€â”€ evaluation_lib.py          
â”‚   â”œâ”€â”€ instructions_registry.py           
â”‚   â”œâ”€â”€ instructions_util.py
â”‚   â””â”€â”€ instructions.py
â”‚
â”œâ”€â”€ evaluation_main.py              # Main evaluation script
â”œâ”€â”€ generate_response.py            # Script to generate model responses
â”œâ”€â”€ run_if.sh                       # Pipeline runner
â”œâ”€â”€ requirements.txt                # Requirements
â””â”€â”€ README.md
```


## âš™ï¸ Environment Setup

```bash
conda create -n ifeval_pp python=3.10
conda activate ifeval_pp
pip install -r requirements.txt
```

## ğŸ“š Dataset Reference

The extended dataset **IFEval++** is derived from the original [IFEval benchmark](https://huggingface.co/datasets/google/IFEval) by Google DeepMind.  
IFEval++ is constructed through systematic cleaning, augmentation, and validation for scalable instruction-following evaluation.

**Original Data**
- `ifeval_original.jsonl` â€“ the original IFEval dataset (see Reference above).
- `ifeval_original_fix.jsonl` - manually cleaned version.


Augmented dataset is generated via the scripts in `data_synthesis/`, including:
- **Rephrasing**: `exp1_rephrase/`
- **Adding distractors**: `exp2_add_distractor/`
- **Constraint/task reconfiguration**: `exp3_constraint-task_reconfiguration/`

After augmentation, run the validity check with:

```bash
python data_synthesis/data_checker.py \
    --input_file /assets/ifeval_original_fix.jsonl \
    --target_dir /path/to/checker_annotation
```


## ğŸ§ª Evaluation / Testing

Model response generation and evaluation can be executed in one step:

```bash
bash run_if.sh
```

**Configuration:**

- `input_data` â€“ path to the input dataset

- `output_dir` â€“ directory to store responses and evaluation results

- `model_name` â€“ model identifier (e.g. gpt-5)

Results are saved under:
```bash
<output_dir>/
â”œâ”€â”€ responses/     # Model outputs
â””â”€â”€ evaluation/    # Evaluation metrics
```


## ğŸ“œ Citation

If you feel that IFEval++ is useful in your research, please cite our paper and the original IFEval preprint:

```bibtex
@article{dong2025revisiting,
  title={Revisiting the Reliability of Language Models in Instruction-Following}, 
  author={Jianshuo Dong and Yutong Zhang and Yan Liu and Zhenyu Zhong and Tao Wei and Chao Zhang and Han Qiu},
  year={2025},
  journal={arXiv preprint arXiv:2512.14754},
}
```


```bibtex
@article{zhou2023instruction,
  title={Instruction-Following Evaluation for Large Language Models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}
```