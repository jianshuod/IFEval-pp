# coding=utf-8
# Copyright 2025 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Binary of evaluating instruction following. See README.md."""

import collections
import dataclasses
import json
from typing import Dict, Optional, Sequence, Union

from src import instructions_registry


@dataclasses.dataclass
class InputExample:
  key: int
  instruction_id_list: list[str]
  prompt: str
  kwargs: list[Dict[str, Optional[Union[str, int]]]]


@dataclasses.dataclass
class OutputExample:
  key: int
  instruction_id_list: list[str]
  prompt: str
  response: str
  follow_all_instructions: bool
  follow_instruction_list: list[bool]


def read_prompt_list(input_jsonl_filename):
  """Read inputs from jsonl."""
  inputs = []
  with open(input_jsonl_filename, "r") as f:
    for l in f:
      example = json.loads(l)
      inputs.append(
          InputExample(key=example["key"],
                       instruction_id_list=example["instruction_id_list"],
                       prompt=example["prompt"],
                       kwargs=example["kwargs"]))
  return inputs


def write_outputs(output_jsonl_filename, outputs):
  """Writes outputs to jsonl."""
  assert outputs
  with open(output_jsonl_filename, "w") as f:
    for o in outputs:
      f.write(
          json.dumps(
              {
                  attr_name: o.__getattribute__(attr_name)
                  for attr_name in [
                      name for name in dir(o) if not name.startswith("_")
                  ]
              }
          )
      )
      f.write("\n")


def test_instruction_following_strict(
    inp,
    prompt_to_response,
):
  """Tests response to see if instrutions are followed."""
  try:
    response = prompt_to_response[inp.prompt]
  except Exception as e:
    # no response can be generated by the model api
    return OutputExample(
      key=inp.key,
      instruction_id_list=inp.instruction_id_list,
      prompt=inp.prompt,
      response="",
      follow_all_instructions=False,
      follow_instruction_list=[False] * len(inp.instruction_id_list),
    )


  r = response.split("\n")
  response_remove_first = "\n".join(r[1:]).strip()

  all_responses = [
      response,
      response_remove_first,
  ]
  
  if "</think>\n\n" in response:
    response = response.split("</think>\n\n")[1]

  instruction_list = inp.instruction_id_list
  is_following_list = []

  for index, instruction_id in enumerate(instruction_list):      
    instruction_cls = instructions_registry.INSTRUCTION_DICT[instruction_id]
    instruction = instruction_cls(instruction_id)
    inp.kwargs[index] = {key: value for key, value in inp.kwargs[index].items() if value is not None}

    try:
      if response.strip() and instruction.check_following(response, **inp.kwargs[index]):
        is_following_list.append(True)
      else:
        is_following_list.append(False)
    except Exception as e:
      # print(index)
      exit()

  return OutputExample(
      key=inp.key,
      instruction_id_list=inp.instruction_id_list,
      prompt=inp.prompt,
      response=response,
      follow_all_instructions=all(is_following_list),
      follow_instruction_list=is_following_list,
  )

def test_instruction_following_loose(
    inp,
    prompt_to_response,
):
  """Tests response for an upper bound for following instructions."""
  response = prompt_to_response[inp.prompt]
  r = response.split("\n")
  response_remove_first = "\n".join(r[1:]).strip()
  response_remove_last = "\n".join(r[:-1]).strip()
  response_remove_both = "\n".join(r[1:-1]).strip()
  revised_response = response.replace("*", "")
  revised_response_remove_first = response_remove_first.replace("*", "")
  revised_response_remove_last = response_remove_last.replace("*", "")
  revised_response_remove_both = response_remove_both.replace("*", "")
  all_responses = [
      response,
      revised_response,
      response_remove_first,
      response_remove_last,
      response_remove_both,
      revised_response_remove_first,
      revised_response_remove_last,
      revised_response_remove_both,
  ]
  instruction_list = inp.instruction_id_list
  is_following_list = []

  for index, instruction_id in enumerate(instruction_list):
    instruction_cls = instructions_registry.INSTRUCTION_DICT[instruction_id]
    instruction = instruction_cls(instruction_id)
    inp.kwargs[index] = {key: value for key, value in inp.kwargs[index].items() if value is not None}
    
    is_following = False
    for r in all_responses:
      if r.strip() and instruction.check_following(r, **inp.kwargs[index]):
        is_following = True
        break

    is_following_list.append(is_following)

  return OutputExample(
      key=inp.key,
      instruction_id_list=inp.instruction_id_list,
      prompt=inp.prompt,
      response=response,
      follow_all_instructions=all(is_following_list),
      follow_instruction_list=is_following_list,
  )


def read_prompt_to_response_dict(input_jsonl_filename):
  """Creates dictionary matching prompt and response."""
  return_dict = {}
  with open(input_jsonl_filename, "r") as f:
    for l in f:
      example = json.loads(l)
      return_dict[example["prompt"]] = example["response"]
  return return_dict


def print_report(outputs):
  """Prints a report on accuracy scores."""

  prompt_total = 0
  prompt_correct = 0
  instruction_total = 0
  instruction_correct = 0

  tier0_total = collections.defaultdict(int)
  tier0_correct = collections.defaultdict(int)

  tier1_total = collections.defaultdict(int)
  tier1_correct = collections.defaultdict(int)

  for example in outputs:
    follow_instruction_list = example.follow_instruction_list
    instruction_id_list = example.instruction_id_list

    prompt_total += 1
    if all(follow_instruction_list):
      prompt_correct += 1

    instruction_total += len(instruction_id_list)
    instruction_correct += sum(follow_instruction_list)

    for instruction_id, followed_or_not in zip(
        instruction_id_list, follow_instruction_list
    ):
      instruction_id = instruction_id.split(":")[0]
      tier0_total[instruction_id] += 1
      if followed_or_not:
        tier0_correct[instruction_id] += 1

    for instruction_id, followed_or_not in zip(
        instruction_id_list, follow_instruction_list
    ):
      tier1_total[instruction_id] += 1
      if followed_or_not:
        tier1_correct[instruction_id] += 1

  print(f"prompt-level: {prompt_correct / prompt_total}")
  print(f"instruction-level: {instruction_correct / instruction_total}")
  print()
  for instruction_id in sorted(tier0_total.keys()):
    accuracy = tier0_correct[instruction_id] / tier0_total[instruction_id]
    print(f"{instruction_id} {accuracy}")
  print()
  for instruction_id in sorted(tier1_total.keys()):
    accuracy = tier1_correct[instruction_id] / tier1_total[instruction_id]
    print(f"{instruction_id} {accuracy}")

def write_log(outputs, json_path):
    """Generates a report on accuracy scores and writes to a JSON file."""

    prompt_total = 0
    prompt_correct = 0
    instruction_total = 0
    instruction_correct = 0

    tier0_total = collections.defaultdict(int)
    tier0_correct = collections.defaultdict(int)

    tier1_total = collections.defaultdict(int)
    tier1_correct = collections.defaultdict(int)

    for example in outputs:
        follow_instruction_list = example.follow_instruction_list
        instruction_id_list = example.instruction_id_list

        prompt_total += 1
        if all(follow_instruction_list):
            prompt_correct += 1

        instruction_total += len(instruction_id_list)
        instruction_correct += sum(follow_instruction_list)

        for instruction_id, followed_or_not in zip(instruction_id_list, follow_instruction_list):
            instruction_id = instruction_id.split(":")[0]
            tier0_total[instruction_id] += 1
            if followed_or_not:
                tier0_correct[instruction_id] += 1

        for instruction_id, followed_or_not in zip(instruction_id_list, follow_instruction_list):
            tier1_total[instruction_id] += 1
            if followed_or_not:
                tier1_correct[instruction_id] += 1
    try:
      result = {
          "prompt_level_accuracy": prompt_correct / prompt_total if prompt_total else None,
          "instruction_level_accuracy": instruction_correct / instruction_total if instruction_total else None,
          "tier0": {},
          "tier1": {},
          "reliable_at_k": calculate_reliability_metrics_for_sampling(outputs)
      }
    except Exception as e:
      try:
        result = {
            "prompt_level_accuracy": prompt_correct / prompt_total if prompt_total else None,
            "instruction_level_accuracy": instruction_correct / instruction_total if instruction_total else None,
            "tier0": {},
            "tier1": {},
            "reliable_at_k": calculate_reliable_at_k(outputs) 
        }
      except Exception as e:
        result = {
            "prompt_level_accuracy": prompt_correct / prompt_total if prompt_total else None,
            "instruction_level_accuracy": instruction_correct / instruction_total if instruction_total else None,
            "tier0": {},
            "tier1": {},
            # "reliable_at_k": calculate_reliable_at_k(outputs) 
        }

    for instruction_id in sorted(tier0_total.keys()):
        acc = tier0_correct[instruction_id] / tier0_total[instruction_id]
        result["tier0"][instruction_id] = acc

    for instruction_id in sorted(tier1_total.keys()):
        acc = tier1_correct[instruction_id] / tier1_total[instruction_id]
        result["tier1"][instruction_id] = acc

    with open(json_path, "w") as f:
        json.dump(result, f, indent=2)



def calculate_reliable_at_k(outputs):

  # ["fill-in alteration", "rephrasing", "distractor", "original"]

  outputs_by_key_all = {}
  outputs_by_key_ct_alteration = {}
  outputs_by_key_rephrasing = {}
  outputs_by_key_distractor = {}
  outputs_by_key_original = {}

  # first only consider the original type
  for output in outputs:
    key = output.key.split(":")[0]
    o_type = output.key.split(":")[1]
    if o_type != "original":
      continue
    else:
      if key not in outputs_by_key_all:
        outputs_by_key_all[key] = []
      outputs_by_key_all[key].append(output)
      if key not in outputs_by_key_original:
        outputs_by_key_original[key] = []
      outputs_by_key_original[key].append(output)
      if key not in outputs_by_key_distractor:
        outputs_by_key_distractor[key] = []
      outputs_by_key_distractor[key].append(output)
      if key not in outputs_by_key_ct_alteration:
        outputs_by_key_ct_alteration[key] = []
      outputs_by_key_ct_alteration[key].append(output)
      if key not in outputs_by_key_rephrasing:
        outputs_by_key_rephrasing[key] = []
      outputs_by_key_rephrasing[key].append(output)

  # group the outputs by key
  for output in outputs:
    key = output.key.split(":")[0]
    o_type = output.key.split(":")[1]
    if o_type == "original":
      continue
    
    if key not in outputs_by_key_all:
      outputs_by_key_all[key] = []
    outputs_by_key_all[key].append(output)

    if o_type == "ct_alteration":
      if key not in outputs_by_key_ct_alteration:
        outputs_by_key_ct_alteration[key] = []
      outputs_by_key_ct_alteration[key].append(output)
    if o_type == "rephrasing":
      if key not in outputs_by_key_rephrasing:
        outputs_by_key_rephrasing[key] = []
      outputs_by_key_rephrasing[key].append(output)
    if o_type == "distractor":
      if key not in outputs_by_key_distractor:
        outputs_by_key_distractor[key] = []
      outputs_by_key_distractor[key].append(output)


  # calculate the reliable at k on all
  reliable_at_k_all_count = 0
  for key, outputs in outputs_by_key_all.items():
    if sum([output.follow_all_instructions for output in outputs]) == 10:
      reliable_at_k_all_count += 1
  reliable_at_k_all = reliable_at_k_all_count / len(outputs_by_key_all)

  # calculate the reliable at k on original
  reliable_at_k_original_count = 0
  for key, outputs in outputs_by_key_original.items():
    if sum([output.follow_all_instructions for output in outputs]) == 1:
      reliable_at_k_original_count += 1
  reliable_at_k_original = reliable_at_k_original_count / len(outputs_by_key_original)


  # calculate the reliable at k on fill-in alteration
  reliable_at_k_ct_alteration_at_2_count = 0
  for key, outputs in outputs_by_key_ct_alteration.items():
    if sum([output.follow_all_instructions for output in outputs[:2]]) == 2:
      reliable_at_k_ct_alteration_at_2_count += 1
  reliable_at_k_ct_alteration_at_2 = reliable_at_k_ct_alteration_at_2_count / len(outputs_by_key_ct_alteration)
  
  # calculate the reliable at k on fill-in alteration
  reliable_at_k_ct_alteration_at_4_count = 0
  for key, outputs in outputs_by_key_ct_alteration.items():
    if sum([output.follow_all_instructions for output in outputs[:4]]) == 4:
      reliable_at_k_ct_alteration_at_4_count += 1
  reliable_at_k_ct_alteration_at_4 = reliable_at_k_ct_alteration_at_4_count / len(outputs_by_key_ct_alteration)

  # calculate the reliable at k on rephrasing
  reliable_at_k_rephrasing_at_2_count = 0
  for key, outputs in outputs_by_key_rephrasing.items():
    if sum([output.follow_all_instructions for output in outputs[:2]]) == 2:
      reliable_at_k_rephrasing_at_2_count += 1
  reliable_at_k_rephrasing_at_2 = reliable_at_k_rephrasing_at_2_count / len(outputs_by_key_rephrasing)
  
  # calculate the reliable at k on rephrasing
  reliable_at_k_rephrasing_at_4_count = 0
  for key, outputs in outputs_by_key_rephrasing.items():
    if sum([output.follow_all_instructions for output in outputs[:4]]) == 4:
      reliable_at_k_rephrasing_at_4_count += 1
  reliable_at_k_rephrasing_at_4 = reliable_at_k_rephrasing_at_4_count / len(outputs_by_key_rephrasing)
  
  # calculate the reliable at k on distractor
  reliable_at_k_distractor_at_2_count = 0
  for key, outputs in outputs_by_key_distractor.items():
    if sum([output.follow_all_instructions for output in outputs[:2]]) == 2:
      reliable_at_k_distractor_at_2_count += 1
  reliable_at_k_distractor_at_2 = reliable_at_k_distractor_at_2_count / len(outputs_by_key_distractor)
  
  # calculate the reliable at k on distractor
  reliable_at_k_distractor_at_4_count = 0
  for key, outputs in outputs_by_key_distractor.items():
    if sum([output.follow_all_instructions for output in outputs[:4]]) == 4:
      reliable_at_k_distractor_at_4_count += 1
  reliable_at_k_distractor_at_4 = reliable_at_k_distractor_at_4_count / len(outputs_by_key_distractor)

  print(f"reliable_at_k_original: {reliable_at_k_original}")
  print(f"reliable_at_k_rephrasing_at_2: {reliable_at_k_rephrasing_at_2}")
  print(f"reliable_at_k_rephrasing_at_4: {reliable_at_k_rephrasing_at_4}")
  print(f"reliable_at_k_distractor_at_2: {reliable_at_k_distractor_at_2}")
  print(f"reliable_at_k_distractor_at_4: {reliable_at_k_distractor_at_4}")
  print(f"reliable_at_k_ct_alteration_at_2: {reliable_at_k_ct_alteration_at_2}")
  print(f"reliable_at_k_ct_alteration_at_4: {reliable_at_k_ct_alteration_at_4}")
  print(f"reliable_at_k_all: {reliable_at_k_all}")
  print(f"relative drop rate: {1 - reliable_at_k_all / reliable_at_k_original}")
  
  results = {}
  results["reliable_at_k_all"] = reliable_at_k_all
  results["reliable_at_k_original"] = reliable_at_k_original
  results["reliable_at_k_ct_alteration_at_2"] = reliable_at_k_ct_alteration_at_2
  results["reliable_at_k_ct_alteration_at_4"] = reliable_at_k_ct_alteration_at_4
  results["reliable_at_k_rephrasing_at_2"] = reliable_at_k_rephrasing_at_2
  results["reliable_at_k_rephrasing_at_4"] = reliable_at_k_rephrasing_at_4
  results["reliable_at_k_distractor_at_2"] = reliable_at_k_distractor_at_2
  results["reliable_at_k_distractor_at_4"] = reliable_at_k_distractor_at_4

  return results


def calculate_reliability_metrics_for_sampling(outputs):

  # cluster all samples sharing the same key
  max_sampling_time = 0

  key_to_raw_results = {}
  for output in outputs:
    key = output.key
    sampling_id = int(output.prompt.split("--kk--")[1])
    if sampling_id > max_sampling_time:
      max_sampling_time = sampling_id

    if key not in key_to_raw_results:
      key_to_raw_results[key] = []
    key_to_raw_results[key].append(output.follow_all_instructions)


  overall_results = {}

  for sampling_id in range(1, max_sampling_time + 2):

    key_to_any_followed = {}
    for key, raw_results in key_to_raw_results.items():
      key_to_any_followed[key] = any(raw_results[:sampling_id])

    outputs_by_key_all = {}
    outputs_by_key_ct_alteration = {}
    outputs_by_key_rephrasing = {}
    outputs_by_key_distractor = {}
    outputs_by_key_original = {}

    # first only consider the original type
    for test_case_key, any_followed in key_to_any_followed.items():
      key = test_case_key.split(":")[0]
      o_type = test_case_key.split(":")[1]
      if o_type != "original":
        continue
      else:
        if key not in outputs_by_key_all:
          outputs_by_key_all[key] = []
        outputs_by_key_all[key].append(any_followed)
        if key not in outputs_by_key_original:
          outputs_by_key_original[key] = []
        outputs_by_key_original[key].append(any_followed)
        if key not in outputs_by_key_distractor:
          outputs_by_key_distractor[key] = []
        outputs_by_key_distractor[key].append(any_followed)
        if key not in outputs_by_key_ct_alteration:
          outputs_by_key_ct_alteration[key] = []
        outputs_by_key_ct_alteration[key].append(any_followed)
        if key not in outputs_by_key_rephrasing:
          outputs_by_key_rephrasing[key] = []
        outputs_by_key_rephrasing[key].append(any_followed)

    # group the outputs by key
    for test_case_key, any_followed in key_to_any_followed.items():
      key = test_case_key.split(":")[0]
      o_type = test_case_key.split(":")[1]
      if o_type == "original":
        continue
      else:
      
        if key not in outputs_by_key_all:
          outputs_by_key_all[key] = []
        outputs_by_key_all[key].append(any_followed)

        if o_type == "ct_alteration":
          if key not in outputs_by_key_ct_alteration:
            outputs_by_key_ct_alteration[key] = []
          outputs_by_key_ct_alteration[key].append(any_followed)
        if o_type == "rephrasing":
          if key not in outputs_by_key_rephrasing:
            outputs_by_key_rephrasing[key] = []
          outputs_by_key_rephrasing[key].append(any_followed)
        if o_type == "distractor":
          if key not in outputs_by_key_distractor:
            outputs_by_key_distractor[key] = []
          outputs_by_key_distractor[key].append(any_followed)


    # calculate the reliable at k on all
    reliable_at_k_all_count = 0
    for key, outputs in outputs_by_key_all.items():
      if sum(outputs) == 10:
        reliable_at_k_all_count += 1
    reliable_at_k_all = reliable_at_k_all_count / len(outputs_by_key_all)

    try:
      # calculate the reliable at k on original
      reliable_at_k_original_count = 0
      for key, outputs in outputs_by_key_original.items():
        if sum(outputs) == 1:
          reliable_at_k_original_count += 1
      reliable_at_k_original = reliable_at_k_original_count / len(outputs_by_key_original)
    except Exception as e:
      reliable_at_k_original = 0

    # calculate the reliable at k on fill-in alteration
    reliable_at_k_ct_alteration_at_2_count = 0
    for key, outputs in outputs_by_key_ct_alteration.items():
      if sum(outputs[:2]) == 2:
        reliable_at_k_ct_alteration_at_2_count += 1
    reliable_at_k_ct_alteration_at_2 = reliable_at_k_ct_alteration_at_2_count / len(outputs_by_key_ct_alteration)
    
    # calculate the reliable at k on fill-in alteration
    reliable_at_k_ct_alteration_at_4_count = 0
    for key, outputs in outputs_by_key_ct_alteration.items():
      if sum(outputs[:4]) == 4:
        reliable_at_k_ct_alteration_at_4_count += 1
    reliable_at_k_ct_alteration_at_4 = reliable_at_k_ct_alteration_at_4_count / len(outputs_by_key_ct_alteration)

    # calculate the reliable at k on rephrasing
    reliable_at_k_rephrasing_at_2_count = 0
    for key, outputs in outputs_by_key_rephrasing.items():
      if sum(outputs[:2]) == 2:
        reliable_at_k_rephrasing_at_2_count += 1
    reliable_at_k_rephrasing_at_2 = reliable_at_k_rephrasing_at_2_count / len(outputs_by_key_rephrasing)
    
    # calculate the reliable at k on rephrasing
    reliable_at_k_rephrasing_at_4_count = 0
    for key, outputs in outputs_by_key_rephrasing.items():
      if sum(outputs[:4]) == 4:
        reliable_at_k_rephrasing_at_4_count += 1
    reliable_at_k_rephrasing_at_4 = reliable_at_k_rephrasing_at_4_count / len(outputs_by_key_rephrasing)
    
    # calculate the reliable at k on distractor
    reliable_at_k_distractor_at_2_count = 0
    for key, outputs in outputs_by_key_distractor.items():
      if sum(outputs[:2]) == 2:
        reliable_at_k_distractor_at_2_count += 1
    reliable_at_k_distractor_at_2 = reliable_at_k_distractor_at_2_count / len(outputs_by_key_distractor)
    
    # calculate the reliable at k on distractor
    reliable_at_k_distractor_at_4_count = 0
    for key, outputs in outputs_by_key_distractor.items():
      if sum(outputs[:4]) == 4:
        reliable_at_k_distractor_at_4_count += 1
    reliable_at_k_distractor_at_4 = reliable_at_k_distractor_at_4_count / len(outputs_by_key_distractor)

    print('=' * 100)
    print(f"sampling_id: {sampling_id}")
    print(f"reliable_at_k_original: {reliable_at_k_original}")
    print(f"reliable_at_k_rephrasing_at_2: {reliable_at_k_rephrasing_at_2}")
    print(f"reliable_at_k_rephrasing_at_4: {reliable_at_k_rephrasing_at_4}")
    print(f"reliable_at_k_distractor_at_2: {reliable_at_k_distractor_at_2}")
    print(f"reliable_at_k_distractor_at_4: {reliable_at_k_distractor_at_4}")
    print(f"reliable_at_k_ct_alteration_at_2: {reliable_at_k_ct_alteration_at_2}")
    print(f"reliable_at_k_ct_alteration_at_4: {reliable_at_k_ct_alteration_at_4}")
    print(f"reliable_at_k_all: {reliable_at_k_all}")
    try:
      print(f"relative drop rate: {1 - reliable_at_k_all / reliable_at_k_original}")
    except Exception as e:
      pass
    results = {}
    results["sampling_id"] = sampling_id
    results["reliable_at_k_all"] = reliable_at_k_all
    results["reliable_at_k_original"] = reliable_at_k_original
    results["reliable_at_k_ct_alteration_at_2"] = reliable_at_k_ct_alteration_at_2
    results["reliable_at_k_ct_alteration_at_4"] = reliable_at_k_ct_alteration_at_4
    results["reliable_at_k_rephrasing_at_2"] = reliable_at_k_rephrasing_at_2
    results["reliable_at_k_rephrasing_at_4"] = reliable_at_k_rephrasing_at_4
    results["reliable_at_k_distractor_at_2"] = reliable_at_k_distractor_at_2
    results["reliable_at_k_distractor_at_4"] = reliable_at_k_distractor_at_4

    overall_results[sampling_id] = results

  return overall_results
